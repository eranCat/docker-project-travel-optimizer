services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: travel-backend
    ports:
      - "8000:8000"
    env_file:
      - .env
    volumes:
      - ./backend:/app
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    volumes:
      - ./frontend:/app
      - /app/node_modules  # to prevent local overwrite of container deps
    ports:
      - "5173:5173"


  ollama:
    build:
      context: .
      dockerfile: backend/ollama.Dockerfile
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: /bin/bash
    command:
      - -c
      - |
        echo "ðŸš€ Starting Ollama server..." &&
        ollama serve &

        echo "ðŸ§  Starting background model pull and warmup..." &&
        (
          sleep 5 &&
          echo "ðŸ“¦ Pulling llama3..." &&
          ollama pull llama3 &&
          echo "ðŸ”¥ Warming up llama3..." &&
          ollama run llama3 "Say hello." || true
        ) &

        wait
    restart: unless-stopped
    healthcheck:
      test: [ "CMD-SHELL", "curl -sf http://localhost:11434/api/tags | grep -q 'llama3' || (echo 'llama3 not ready' && exit 1)" ]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 30s

volumes:
  ollama_data:
