services:
  llm_service:
    build:
      context: ./llm_service
      dockerfile: Dockerfile
    container_name: llm-service
    ports:
      - "8002:8000"
    env_file:
      - ./llm_service/.env
    volumes:
      - ./llm_service:/app
      - ./models:/app/models 
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  maps_service:
    build:
      context: ./maps_service
      dockerfile: Dockerfile
    container_name: maps-service
    ports:
      - "8001:8000"
    volumes:
      - ./maps_service/app:/app/app
      - ./models:/app/models
    env_file:
      - ./maps_service/.env
    restart: unless-stopped
    depends_on:
      llm_service:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: travel-backend
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./models:/app/models
    restart: unless-stopped
    depends_on:
      maps_service:
        condition: service_healthy
      llm_service:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    volumes:
      - ./frontend:/app
      - /app/node_modules # to prevent local overwrite of container deps
    ports:
      - "5173:5173"
    depends_on:
      backend:
        condition: service_healthy

  tests:
    build:
      context: .
      dockerfile: tests/Dockerfile.tests
    container_name: travel-tests
    depends_on:
      backend:
        condition: service_healthy
      maps_service:
        condition: service_healthy
      llm_service:
        condition: service_healthy